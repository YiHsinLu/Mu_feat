```{r}
token_split = function(df, g, id=rownames(df)){
#parameters
  n = nrow(df) #numbers of row
  colnames(df) = c("id", "word") 
  
#g=1
#text mining to take the tokens from column--"word" for each row
df_1g = df%>%
  unnest_tokens(words,word)#turn a block into few tokens
colnames(df_1g)=c("id","word")
data(stop_words)
df_1g = df_1g%>%
  anti_join(stop_words)#delete the stop words

#token list for one-gram
token1 = df_1g%>%
  group_by()%>%
  count(word,sort=TRUE)%>%
  ungroup()#count the number of the token showed

#g=2
df_2g <- df %>%
  unnest_tokens(words, word, token = "ngrams", n = 2)
colnames(df_2g)=c("id","word")
data(stop_words)
df_2g = df_2g%>%
  anti_join(stop_words)#delete the stop words

#token list for two-grams
token2 = df_2g%>%
  group_by()%>%
  count(word,sort=TRUE)%>%
  ungroup()#count the number of the token showed

#list function
lappend <- function (lst, ...){
  lst <- c(lst, list(...))
  return(lst)
}

#list of token in each musicians(one-gram)
token_list_1g = list()
for(i in id){
  token_pr = subset(df_1g,id==i)[2]
  token_list_1g = lappend(token_list_1g, token_pr)
  token_pr = c()
}

#list of token in each musicians(two-grams)
token_list_2g = list()
for(i in id){
  token_pr = rbind(subset(df_1g,id==i)[2], subset(df_2g,id==i)[2])
  token_list_2g = lappend(token_list_2g, token_pr)
  token_pr = c()
}

if(g==1){
  return(list(id=id, token_list1=token1$word, df_token_lst=token_list_1g))
}else{
  return(list(id=id, token_list1=token1$word, token_list2=token2$word, df_token_lst=token_list_2g))
}
}
```

```{r}
# Create Training and Testing data

mu_feat = read.csv("D:/NDHU_Master/110-2/KML/Project/Data_sets/data.csv", row.names = 1)

X_mufeat = mu_feat[,-29]
Y_mufeat = as.data.frame(mu_feat[,'label'])
colnames(Y_mufeat) = 'label'
feat = colnames(X_mufeat)

multiSample = function(n,g,k){
  # totally n members
  # for g groups
  # take k members from each group
  if(n%%g==0){
    ms = c()
    ng = n/g
    for(i in 1:g){
      init = (i-1)*ng+1
      ms = cbind(ms, sample(init:(i*ng), k, replace = FALSE))
    }
    return(sort(ms))
  }else{
    print('Error: n could not divided by g')
  }
}







Y_nu = as.data.frame(sapply(factor(Y_mufeat$label), unclass))
colnames(Y_nu) = 'ladel'
rownames(Y_nu) = rownames(Y_mufeat) = rownames(X_mufeat)

df_Y = cbind(rownames(mu_feat), Y_mufeat)

ts.y = token_split(df = df_Y, g=1)

genre = ts.y$token_list1
```



```{r}
library(caret)

#define one-hot encoding function
dummy <- dummyVars(" ~ .", data=Y_mufeat)

#perform one-hot encoding on data frame
y_ohencoding <- data.frame(predict(dummy, newdata=Y_mufeat))

#view final data frame
y_ohencoding

```

```{r}
colnames(y_ohencoding)=genre
y_ohencoding
```


#data matrix
```{r}
data_ohencoding = as.data.frame(cbind(X_mufeat, y_ohencoding))
data_ohencoding
```

```{r}
set.seed(608*320)
sel = multiSample(1000,10,80)

data_ohencoding_train = data_ohencoding[sel,]
data_ohencoding_test = data_ohencoding[-sel,]
```

#blue
```{r}
set.seed(608*320)
library(e1071)
gammalist <- c(0.005,0.01,0.015,0.02,0.025,0.03,0.035,0.04,0.045,0.05)

svm_blues = tune.svm( ~., data = data_ohencoding_train[,c(1:28,29)], 
                 kernel='radial', cost=2^(-1:5), gamma = gammalist)

summary(svm_blues)
```

```{r}
y_pre = predict(svm_blues$best.model, data_ohencoding_test[,1:28])
y = as.factor(data_ohencoding_test$blues)


Con_Matr = confusionMatrix(y_pre,y)


Con_Matr
```

```{r}
set.seed(608*320)
library(e1071)
gammalist <- c(0.005,0.01,0.015,0.02,0.025,0.03,0.035,0.04,0.045,0.05)


for(i in 1:length(genre)){
  gr = genre[i]
  svm = tune.svm(as.factor() ~., data = data_ohencoding_train[,c(i,11:38)], 
                 kernel='radial', cost=2^(-1:5), gamma = gammalist)
}

```